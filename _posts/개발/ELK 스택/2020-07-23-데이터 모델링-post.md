---

title: "데이터 모델링"
date: 2020-07-23 15:44:00 -0400
categories: ELK스택

## 매핑 API 이해하기

-   한 번 생성된 매핑의 타입은 변경 될 수 없음
-   타입을 변경하려면 인덱스를 삭제한 후 다시 생성하거나 매핑을 다시 정의해야 함

## 매핑 인덱스 만들기

```
PUT /movie
{
    "settings": {
        "number_of_shards": 5,
        "number_of_replicas": 1
    },
    "mappings": {
        "_doc": {
            "properties": {
                "movieCd": { "type": "keyword" },
                "movieNm": { "type": "text"
                               , "analyzer": "standard" },
                "movieNmEn": { "type": "text", "analyzer": "standard" },
                "prdYear": { "type": "integer" },
                "openDt": { "type": "date" },
                "typeNm": { "type": "keyword" },
                "prdtStatNm": { "type": "keyword" },
                "nationAlt": { "type": "keyword" },
                "genreAlt": { "type": "keyword" },
                "repNationNm": { "type": "keyword" },
                "repGenreNm": { "type": "keyword" },
                "companies": {
                  "properties": {
                    "companyCd": { "type": "keyword" },
                    "companyNm": { "type": "keyword" }
                  }
                },
                "directors":{
                  "properties": {
                    "peopleNm": { "type": "keyword" }
                  }
                }
            }
        }
    }
}
```

## 매핑 파라미터

-   색인할 필드의 데이터를 어떻게 저장할지에 대한 다양한 옵션을 제공<br>(Mapping parameters 상세정보)
    | `매핑 파라미터` | `설명` |
    | :---: | :---- |
    | properties | 오브젝트 타입이나 중첩 타입의 스키마를 정의할 때 사용되는 옵션으로 필드의 타입을 매핑함 |
    | analyzer | 인덱스와 검색에 사용할 형태소 분석기를 선택. 기본값은 Standard Analyzer |
    | boost | 필드의 가중치로 검색 결과 정렬에 영향을 줌. 기본값은 1.0을 1보다 크면 높게 오르고 적으면 점수가 낮게 오름 |
    | fielddata | 정렬, 집계, 스크립트 등에서 메모리에 저장된 필드 데이터를 사용할지를 설정. 기본값은 false |
    | index | 필드값을 색인할지를 결정. 기본값은 true |
    | norms | 유사도 점수를 산정할 때 필드 길이를 고려할지를 결정. 기본값은 true |
    | store | 필드 값을 필드와 별도로 \_source에 저장하고 검색 가능하게 할지를 설정. 기본값은 false |
    | search_analyzer | 검색에 사용할 형태소 분석기를 선택 |
    | similarity | 유사도 점수를 구하는 알고리즘을 선택. 기본값 BM25 |
    | term_vector | Analyzed 필드에 텀백터를 저장할지를 결정. 기본값 no |
    | doc_values | 필드를 메모리에 로드해 캐시로 사용. 기본값 true |
    | null_value | 기본적으로 엘라스틱서치는 데이터의 값이 없으면 필드를 생성하지 않음. 데이터의 길이 없는 경우 null로 필드의 값을 대체할지를 설정 |
    | normalizer | keyword 데이터 타입의 경우 원문을 기준으로 문서가 색인되기 때문에 cafe, Cafe, Café는 서로 다른 문서로 인식됨. 하지만 해당 유형을 nomalizer를 통해 분석기에 asciifolding과 같은 필터를 사용하면 같은 데이터로 인식되게 할 수 있음 |
    | coerce | 색인시 자동 변환을 허용할지 여부를 설정. integer 타입의 필드 "10"값을 -> 10 자동 형변환 |
    | copy_to | 매핑 파라미터로 추가한 필드의 값을 지정한 필드로 복사 |
    | dynamic | 매핑에 필드를 추가할 때 동적으로 생성할지, 생성하지 않을지 결정 |
    | enabled | 검색 결과에는 포함하지만 색인은 하고 싶지 않은 경우 사용 |
    | format | 날짜/시간을 문자열로 표시 |
    | ignore_above | 필드에 저장되는 문자열이 지정한 크기를 넘으면 빈 값으로 색인 |
    | ignore_malformed | 잘못된 데이터 타입을 색인시 해당 필드만 무시되고 문서는 색인할 수 있음 |
    | fileds | 다중 필드를 설정할 수 있는 옵션. 기본 필드는 전문 검색을하고 필드 안의 추가 필드는 집계용으로 사용할 수 있음 |

## 필드 데이터 타입

-   매핑 설정을 위해서 엘라스틱서치에서 제공하는 데이터 타입(Field datatype 상세정보)

| `데이터 타입` | `설명`                                                                                                                                                 |
| :-----------: | ------------------------------------------------------------------------------------------------------------------------------------------------------ |
|    Keyword    | 별도의 분석기를 거치지 않고 원문 그대로 색인. Keyword 데이터 타입을 사용해야하는 경우(검색 시 필터링되는 항목, 정렬이 필요한 항목, 집계해야 하는 항목) |
|     Text      | 색인 시 지정된 분석기가 칼럼의 데이터를 문자열 데이터로 인식하고 이를 분석. 기본적으로 Standard Analyzer를 사용                                        |
|     Array     | 문자열이나 숫자처럼 일반적인 값을 지정할 수도 있지만 객체 형태로도 정의. Array 타입에 저장되는 값은 모두 같은 타입으로만 구성                          |
|    Numeric    | 숫자 데이터 타입은 여러 가지 종류가 제공(long, integer, short, byte, double, float, half_float)                                                        |
|     Date      | JSON 포멧에서 문자열로 처리                                                                                                                            |
|     Range     | 범위가 있는 데이터 타입을 저장할 때 사용. 숫자뿐 아니라 IP에 대한 범위도 정의 할 수 있음                                                               |
|    Boolean    | 참과 거짓 두 논리값을 가지는 데이터 타입. 문자열 표현도 가능(참 : true, "true", 거짓: false, "false")                                                  |
|   Geo-Point   | 위도, 경도 등 위치 정보를 담은 데이터 타입을 저장할 때 사용                                                                                            |
|      IP       | 데이터 타입                                                                                                                                            |  |
|      IP       | 주소와 같은 데이터 저장하는 데 사용. IPv4, IPv6 모두 지정할 수 있음                                                                                    |
|    Object     | 문서는 내부 계층적으로 포함할 수 있음                                                                                                                  |
|    Nested     | Object 객체 배열을 독립적으로 색인하고 질의하는 형태의 데이터 타입                                                                                     |

## 동의어

### 동의어를 추가하는 2가지 방식

-   동의어를 매핑 설정 정보에 미리 파라미터로 등록하는 방식 <- 운영중에 매핑 정보를 바꾸기 어렵기 때문에 실무에서는 잘 안 쓰임
-   특정 파일을 별도로 생성해서 관리하는 방식

### 동의어 사전 만들기

-   엘라스틱서치가 설치된 서버 아래의 config 디렉토리에 생성
-   config 디렉터리 아래에 analysis라는 디렉토리를 만들고 synonym.txt라는 파일을 생성<br>경로 : <엘라스틱서치 설치 디렉토리>/config/analysis/synonym.txt
-   동의어를 추가할 때 단어를 쉼표로 분리해 등록
-   동의어 사전은 실시간으로 적용되지 않음
-   수정된 동의어를 적용하고 싶다면 해당 동의어 사전을 사용하고 있는 인덱스를 Reload해야 함
-   검색 시점에는 사전의 내용이 변경되더라도 해당 내용 반영
-   색인 시점에 동의어 사전이 사용됐다면 사전의 내용이 변경되더라도 색인이 변경되지는 않음

# Text Analysis

## ElasticSearch Text Analysis ?

-   검색어를 추출하기 위한 프로세스
-   Text Analysis 를 통해, ElasticSearch는 검색어로 부터 전체 Text 검색이 가능
-   여기서 검색은 일치하는 Text 항목이 아닌 모든 관련 Text 결과 반환

## Analyzer Pipeline

### Char Filters

-   문장을 분석하기 전에 입력 텍스트에 대해 특정한 단어를 변경하거나 HTML과 같은 태그를 제거하는 역할
-   해당 내용은 텍스트를 토큰화하기 전의 전처리 과정

### Tokenizer

-   텍스트를 어떻게 나눌 것인지를 정의
-   텍스트를 설정된 기준에 따라 검색어 토큰으로 분리 역할

### Token Filters

-   토큰화된 단어를 하나씩 필터링해서 사용자가 원하는 토큰으로 변환
-   ex) 불필요한 단어를 제거하거나 동의어 사전을 만들어 단어를 추가하거나 영문 단어를 소문자로 변환하는 등의 작업을 수행 할 수 있음
-   분석 과정에서 제일 중요한 단계

## Char Filters, Tokenizer, Token Filters 종류

|    `구분`     |         `종류`         | `설명`                                                                                                                                                          | `옵션(파라미터)` | `설명`                                                                                                                                                       |
| :-----------: | :--------------------: | :-------------------------------------------------------------------------------------------------------------------------------------------------------------- | :--------------: | :----------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Char Filters  | html_strip_char_filter | 문장에서 HTML을 제거하는 전처리 필터                                                                                                                            |   escaped_tags   | 특정 태그만 삭제한다. 기본값으로 HTML 태그 전부 삭제                                                                                                         |
|   Tokenizer   |        standard        | 엘라스틱서치에서 일반적으로 사용하는 Tokenizer, 대부분의 기호를 만나면 토근으로 분리                                                                            | max_token_length | 최대 토큰 길이를 초과하는 경우 해당 간격으로 토큰을 분할한다. 기본값은 255                                                                                   |
|               |       whitespace       | 공백을 만나면 텍스트를 토큰화                                                                                                                                   | max_token_length | 최대 토큰 길이를 초과하는 경우 해당 간격으로 토큰을 분할한다. 기본값은 255                                                                                   |
|               |    ngram_tokenizer     | 한 글자씩 토큰화, 특정 문자를 지정하여 지정된 문자의 목록 중, 하나를 만날 때마다 단어 분리 가능                                                                 |     min_gram     | Ngram을 적용할 문자의 최소 길이, 기본값 : 1                                                                                                                  |
|               |                        |                                                                                                                                                                 |     max_gram     | Ngram을 적용할 문자의 최대 길이, 기본값 : 2                                                                                                                  |
|               |                        |                                                                                                                                                                 |   token_chars    | 토큰에 포함할 문자열을 지정, 다음과 같은 다양한 옵션 제공<br>letter (문자)<br>digit (숫자)<br>whitespace (공백)<br>punctuation (구두점)<br>symbol (특수기호) |
|               |  edge_ngram_tokenizer  | 지정된 문자의 목록 중 하나를 만날 때마다 시작 부분을 고정시켜 단어를 분리하는 방식                                                                              |     min_gram     | Ngram을 적용할 문자의 최소 길이, 기본값 : 1                                                                                                                  |
|               |                        |                                                                                                                                                                 |     max_gram     | Ngram을 적용할 문자의 최대 길이, 기본값 : 2                                                                                                                  |
|               |                        |                                                                                                                                                                 |   token_chars    | 토큰에 포함할 문자열을 지정, 다음과 같은 다양한 옵션 제공<br>letter (문자)<br>digit (숫자)<br>whitespace (공백)punctuation (구두점)<br>symbol (특수기호)     |
|               |        keyword         | 텍스트를 하나의 토큰으로 생성                                                                                                                                   |   buffer_size    | 텀을 버퍼로 읽어 들일 문자 수를 지정, 기본값은 256                                                                                                           |
| Token Filters |      asciifolding      | 아스키 코드에 해당하는 127개의 알파벳, 숫자, 기호에 해당하지 않는 경우 문자를 ASCII 요소로 변경                                                                 |                  |                                                                                                                                                              |
|               |       lowercase        | 토큰을 구성하는 전체 문자열을 소문자로 변환                                                                                                                     |                  |                                                                                                                                                              |
|               |       uppercase        | 토큰을 구성하는 전체 문자열을 대문자로 변환                                                                                                                     |                  |                                                                                                                                                              |
|               |      stop_filter       | 불용어로 등록할 사전을 구축해서 사용하는 필터를 의미<br>인덱스로 만들고 싶지 않거나 검색되지 않게 하고 싶은 단어를 등록해서 해당 단어에 대한 불용어 사전을 구축 |    stopwords     | 불용어를 매핑에 직접 등록해서 사용                                                                                                                           |
|               |                        |                                                                                                                                                                 |  stopwords_path  | 불용어 사전이 존재하는 경로 지정, 해당 경로는 엘라스틱서치 서버가 있는 config 폴더 안에 생성                                                                 |
|               |                        |                                                                                                                                                                 |   ignore_case    | true로 지정할 경우 모든 단어를 소문자로 변경해서 저장, 기본값은 false                                                                                        |
|               |   stemmer_eng_filter   | Stemming 알고리즘을 사용해 토큰을 변형                                                                                                                          |       name       | english, light_english, minimal_english, possessive_english, porter2, lovins 등 다른 나라의 언어도 사용 가능, 한글은 지원 X                                  |
|               |     synonym_filter     | 동의어 처리 필터                                                                                                                                                |     synonyms     | 동의어로 사용할 단어 등록                                                                                                                                    |
|               |                        |                                                                                                                                                                 |  synonyms_path   | 파일로 관리할 경우 엘라스틱처시 서버의 config 폴더 아래 생성                                                                                                 |
|               |          trim          | 앞뒤 공백을 제거하는 필터                                                                                                                                       |                  |
